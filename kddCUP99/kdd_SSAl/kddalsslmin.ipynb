{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 替换为实际数据集路径\n",
    "labelled_data_path = 'kddcup_1_percent'\n",
    "unlabelled_data_path = 'kddcup_1_percent.unlabeled'\n",
    "except_path = 'kddcup_1_percent'\n",
    "\n",
    "# 读取有标签的数据集\n",
    "df_labelled = pd.read_csv(labelled_data_path, header=None)\n",
    "\n",
    "# 读取无标签的数据集\n",
    "df_unlabelled = pd.read_csv(unlabelled_data_path, header=None)\n",
    "df_except = pd.read_csv(except_path, header=None)\n",
    "\n",
    "# 定义列名\n",
    "columns = [\"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\", \"dst_bytes\", \n",
    "           \"land\", \"wrong_fragment\", \"urgent\", \"hot\", \"num_failed_logins\", \n",
    "           \"logged_in\", \"num_compromised\", \"root_shell\", \"su_attempted\", \n",
    "           \"num_root\", \"num_file_creations\", \"num_shells\", \"num_access_files\", \n",
    "           \"num_outbound_cmds\", \"is_host_login\", \"is_guest_login\", \"count\", \n",
    "           \"srv_count\", \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \n",
    "           \"srv_rerror_rate\", \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\", \n",
    "           \"dst_host_count\", \"dst_host_srv_count\", \"dst_host_same_srv_rate\", \n",
    "           \"dst_host_diff_srv_rate\", \"dst_host_same_src_port_rate\", \n",
    "           \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\", \n",
    "           \"dst_host_srv_serror_rate\", \"dst_host_rerror_rate\", \n",
    "           \"dst_host_srv_rerror_rate\", \"label\"]\n",
    "\n",
    "df_labelled.columns = columns\n",
    "df_unlabelled.columns = columns[:-1]  # 无标签数据集没有label列\n",
    "df_except.columns = columns\n",
    "\n",
    "# 获取分类特征的所有种类\n",
    "categorical_columns = [\"protocol_type\", \"service\", \"flag\"]\n",
    "\n",
    "# 为所有分类特征创建一个LabelEncoder对象\n",
    "le_dict = {col: LabelEncoder() for col in categorical_columns}\n",
    "\n",
    "# Fit LabelEncoder对象于有标签和无标签数据集中\n",
    "for col in categorical_columns:\n",
    "    le = le_dict[col]\n",
    "    le.fit(pd.concat([df_labelled[col], df_unlabelled[col], df_except[col]]))\n",
    "\n",
    "# 对有标签数据集进行编码\n",
    "for col in categorical_columns:\n",
    "    le = le_dict[col]\n",
    "    df_labelled[col] = le.transform(df_labelled[col])\n",
    "\n",
    "# 对无标签数据集进行编码\n",
    "for col in categorical_columns:\n",
    "    le = le_dict[col]\n",
    "    df_unlabelled[col] = le.transform(df_unlabelled[col])\n",
    "\n",
    "# 对df_except进行编码\n",
    "for col in categorical_columns:\n",
    "    le = le_dict[col]\n",
    "    df_except[col] = le.transform(df_except[col])\n",
    "\n",
    "# 将标签编码\n",
    "df_labelled['label'] = df_labelled['label'].apply(lambda x: 1 if x != 'normal.' else 0)\n",
    "df_except['label'] = df_except['label'].apply(lambda x: 1 if x != 'normal.' else 0)\n",
    "\n",
    "# 特征标准化\n",
    "scaler = StandardScaler()\n",
    "df_labelled[df_labelled.columns[:-1]] = scaler.fit_transform(df_labelled[df_labelled.columns[:-1]])\n",
    "df_unlabelled[df_unlabelled.columns] = scaler.transform(df_unlabelled[df_unlabelled.columns])\n",
    "df_except[df_except.columns[:-1]] = scaler.transform(df_except[df_except.columns[:-1]])\n",
    "\n",
    "# 划分有标签数据集的训练集和测试集\n",
    "X = df_labelled.drop(columns=['label'])\n",
    "y = df_labelled['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.99, random_state=42)\n",
    "\n",
    "# 使用训练集中所有的有标签数据\n",
    "X_train_labelled = X_train\n",
    "y_train_labelled = y_train\n",
    "\n",
    "# 无标签数据\n",
    "X_train_unlabelled = df_unlabelled\n",
    "\n",
    "# 处理后的df_except\n",
    "X_except = df_except.drop(columns=['label'])\n",
    "y_except = df_except['label']\n",
    "\n",
    "# 显示数据集状态\n",
    "print(f\"Labelled Data Shape: {df_labelled.shape}\")\n",
    "print(f\"Unlabelled Data Shape: {df_unlabelled.shape}\")\n",
    "print(f\"Except Data Shape: {df_except.shape}\")\n",
    "\n",
    "# 显示分割后数据集的形状\n",
    "print(f\"X_train Labelled Shape: {X_train_labelled.shape}\")\n",
    "print(f\"y_train Labelled Shape: {y_train_labelled.shape}\")\n",
    "print(f\"X_test Shape: {X_test.shape}\")\n",
    "print(f\"y_test Shape: {y_test.shape}\")\n",
    "print(f\"X_except Shape: {X_except.shape}\")\n",
    "print(f\"y_except Shape: {y_except.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# 数据概述与统计描述\n",
    "print(\"Labelled Data Statistics:\")\n",
    "print(df_labelled.describe())\n",
    "print(\"\\nUnlabelled Data Statistics:\")\n",
    "print(df_unlabelled.describe())\n",
    "print(\"\\nExcept Data Statistics:\")\n",
    "print(df_except.describe())\n",
    "\n",
    "# 特征分布可视化\n",
    "def plot_feature_distribution(data, columns):\n",
    "    for col in columns:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.histplot(data[col], kde=True)\n",
    "        plt.title(f\"Distribution of {col}\")\n",
    "        plt.show()\n",
    "\n",
    "# 绘制有标签数据的特征分布\n",
    "plot_feature_distribution(df_labelled, df_labelled.columns[:-1])\n",
    "\n",
    "# 标签分布\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='label', data=df_labelled)\n",
    "plt.title('Label Distribution in Labelled Data')\n",
    "plt.show()\n",
    "\n",
    "# 计算相关性矩阵\n",
    "corr_matrix = df_labelled.corr()\n",
    "\n",
    "# 绘制相关性矩阵热图\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix Heatmap')\n",
    "plt.show()\n",
    "\n",
    "# 小提琴图示例\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.violinplot(x=\"label\", y=\"duration\", data=df_labelled)\n",
    "plt.title(\"Violin Plot of Duration by Label\")\n",
    "plt.show()\n",
    "\n",
    "# 注意：根据实际需要调整绘图参数和数据选择\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 初始模型训练\n",
    "clf = SVC(probability=True, random_state=42)\n",
    "clf.fit(X_train_labelled, y_train_labelled)\n",
    "\n",
    "# 自训练迭代\n",
    "max_iterations = 10\n",
    "confidence_threshold = 0.9\n",
    "low_confidence_sample_size = 100  # 定义一个合理的低置信度样本数量\n",
    "\n",
    "# 自训练（半监督学习）迭代\n",
    "for iteration in range(max_iterations):\n",
    "    # 半监督学习部分\n",
    "    y_unlabelled_pred = clf.predict(X_train_unlabelled)\n",
    "    y_unlabelled_proba = clf.predict_proba(X_train_unlabelled)\n",
    "    \n",
    "    # 选择高置信度样本\n",
    "    high_confidence_indices = np.where(np.max(y_unlabelled_proba, axis=1) >= confidence_threshold)[0]\n",
    "    \n",
    "    if len(high_confidence_indices) == 0:\n",
    "        break\n",
    "    \n",
    "    X_high_confidence = X_train_unlabelled.iloc[high_confidence_indices]\n",
    "    y_high_confidence = y_unlabelled_pred[high_confidence_indices]\n",
    "    \n",
    "    # 将高置信度样本添加到有标签的训练集中\n",
    "    X_train_labelled = pd.concat([X_train_labelled, X_high_confidence])\n",
    "    y_train_labelled = pd.concat([y_train_labelled, pd.Series(y_high_confidence)])\n",
    "    \n",
    "    # 从无标签数据集中删除高置信度样本\n",
    "    X_train_unlabelled = X_train_unlabelled.drop(X_high_confidence.index)\n",
    "    \n",
    "    # 主动学习部分\n",
    "    # 确保低置信度样本数量不超过未标记数据的大小\n",
    "    if len(X_train_unlabelled) < low_confidence_sample_size:\n",
    "        low_confidence_sample_size = len(X_train_unlabelled)\n",
    "    \n",
    "    # 重新计算未标记样本的预测概率\n",
    "    y_unlabelled_proba = clf.predict_proba(X_train_unlabelled)\n",
    "    \n",
    "    # 选择低置信度样本进行标注\n",
    "    low_confidence_indices = np.argsort(np.max(y_unlabelled_proba, axis=1))[:low_confidence_sample_size]\n",
    "    X_low_confidence = X_train_unlabelled.iloc[low_confidence_indices]\n",
    "    \n",
    "    # 从df_except中获取标签\n",
    "    X_low_confidence_features = X_low_confidence.reset_index(drop=True)\n",
    "    y_low_confidence_labels = df_except[df_except.drop(columns=['label']).apply(tuple, axis=1).isin(X_low_confidence_features.apply(tuple, axis=1))]['label'].values\n",
    "    \n",
    "    # 检查是否找到了所有低置信度样本的标签\n",
    "    if len(y_low_confidence_labels) < low_confidence_sample_size:\n",
    "        print(f\"Warning: Only found {len(y_low_confidence_labels)} labels for low confidence samples.\")\n",
    "        low_confidence_sample_size = len(y_low_confidence_labels)\n",
    "        X_low_confidence = X_low_confidence.iloc[:low_confidence_sample_size]\n",
    "    \n",
    "    # 添加低置信度样本及其标签到训练集中\n",
    "    X_train_labelled = pd.concat([X_train_labelled, X_low_confidence])\n",
    "    y_train_labelled = pd.concat([y_train_labelled, pd.Series(y_low_confidence_labels)])\n",
    "    \n",
    "    # 从无标签数据集中删除低置信度样本\n",
    "    X_train_unlabelled = X_train_unlabelled.drop(X_low_confidence.index)\n",
    "    \n",
    "    # 重新训练模型\n",
    "    clf.fit(X_train_labelled, y_train_labelled)\n",
    "\n",
    "# 模型评估\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "actl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
